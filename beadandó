# A kód a Starbucks (sbux) és SPDR S&P 500 ETF (spy) historikus adataival dolgozik.
# Az adatok az elmúlt 5 évből vannak a 2018.06.18.-2023.06.17.-ig terjedő időszakra.

# Packagek importálása
import random
import pandas as pd
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

# seed beállítása és adatok beolvasása
random.seed(31415)
spy = pd.read_csv('SPY.csv', parse_dates=['Date'])
sbux = pd.read_csv('SBUX.csv', parse_dates=['Date'])
sbux.head()
spy.head()

#Starbucks ábra
plt.plot(sbux.Date, sbux.Close)
plt.xlabel('Date')
plt.ylabel('SBUX')
plt.title('Starbucks stock value')

plt.tight_layout()

plt.show()

# SPY ábra

plt.plot(spy.Date, spy.Close)
plt.xlabel('Date')
plt.ylabel('SBUX')
plt.title('S&P 500 ETF value')

plt.tight_layout()

plt.show()

#1. feladat
#Napi hozamok kiszámítása
spy_ret = spy['Adj Close'].pct_change().dropna()
sbux_ret = sbux['Adj Close'].pct_change().dropna()
spy_ret.head()

# Portfólió hozamának és varianciájának kiszámítása különböző súlyok mellett (0-1)
weights = np.arange(0, 1.1, 0.1)

portfolio_variances = []
portfolio_returns = []
portfolio_avg_ret = []

for w in weights:
    portfolio_returns.append(w * spy_ret + (1 - w) * sbux_ret)
    portfolio_variances.append(portfolio_returns[-1].var())
    portfolio_avg_ret.append(w * np.mean(spy_ret) + (1 - w) * np.mean(sbux_ret))

portfolio_avg_ret = pd.DataFrame(portfolio_avg_ret, columns = ['Returns'])
portfolio_avg_ret.head()

# A hozamok ábrája
plt.subplot(1, 2, 1) 
plt.plot(weights, portfolio_avg_ret, marker='o')
plt.xlabel('Portfolio Weight')
plt.ylabel('Returns')
plt.title('Portfolio Returns')

# A varianciák ábrája
plt.subplot(1, 2, 2) 
plt.plot(weights, portfolio_variances, marker='o')
plt.xlabel('Portfolio Weight')
plt.ylabel('Variances')
plt.title('Portfolio Variances')

plt.tight_layout()

plt.show()

# Az ábrákon az látszik, hogy ahogyan csökkentjük a Starbucks részvény súlyát a portfóliónkban úgy csökken a varianca,
# azonban a hozamok is úgy csökkennek.

# Autograde function
def calculate_historical_var(df_portfolio_returns, alpha):
    sorted_returns = np.sort(df_portfolio_returns)
    var_idx = int(np.floor(alpha * len(sorted_returns)))
    return sorted_returns[var_idx]

# Példa a függvény használatára
alpha = 0.95  # Confidence level for VaR

for i in range(len(weights)):
    var = calculate_historical_var(portfolio_returns[i], alpha)
    print(f"For weight {weights[i]:.1f}, VaR at {alpha*100}% confidence level: {var:.4f}")

# 2.feladat

expected_return_spy = np.mean(spy_ret)
expected_return_sbux = np.mean(sbux_ret)

volatility_spy = spy_ret.var()
volatility_sbux = sbux_ret.var()

correlation = 0.5

weight_spy = 1 / volatility_spy
weight_sbux = 1 / volatility_sbux
total_weight = weight_spy + weight_sbux
weight_spy /= total_weight
weight_sbux /= total_weight

print(weight_spy, weight_sbux)

# Autograde Function: simulated_returns
def simulated_returns(expected_return, volatility, correlation, numOfSim):
    # Generate random returns for two assets based on the provided parameters
    returns = np.random.multivariate_normal(
        mean=[expected_return, expected_return],
        cov=[[volatility**2, correlation * volatility**2],
             [correlation * volatility**2, volatility**2]],
        size=numOfSim
    )
        # Extract returns for each asset
    asset1_returns = returns[:, 0]
    asset2_returns = returns[:, 1]
    
    # Calculate the inverse volatility weights
    total_volatility = volatility + volatility
    weight1 = volatility / total_volatility
    weight2 = volatility / total_volatility
    
    # Calculate the simulated portfolio returns
    portfolio_returns = weight1 * asset1_returns + weight2 * asset2_returns
    
    return portfolio_returns

# Hozam szimuláció a két eszközre
num_of_simulations = 10000
simulated_returns_spy = simulated_returns(expected_return_spy, volatility_spy, 1.0, num_of_simulations)
simulated_returns_sbux = simulated_returns(expected_return_sbux, volatility_sbux, 1.0, num_of_simulations)

# Portfólió létrehozása
portfolio_returns = weight_spy * simulated_returns_spy + weight_sbux * simulated_returns_sbux

portfolio_returns[1]

for i in range(len(portfolio_returns)):
  portfolio_returns_1.append(portfolio_returns[i][1])

# VaR
alpha = 0.05  # Confidence level for VaR
sorted_returns = np.sort(portfolio_returns)
var_idx = int(np.floor(alpha * num_of_simulations))
var = sorted_returns[var_idx]

# Különböző korrelációk melletti VaR
VaRs = []

correlations = np.arange(0, 1.1, 0.05)
for corr in correlations:
    # Hozam szimuláció a két eszközre
    num_of_simulations = 10000
    simulated_returns_spy = simulated_returns(expected_return_spy, volatility_spy, corr, num_of_simulations)
    simulated_returns_sbux = simulated_returns(expected_return_sbux, volatility_sbux, corr, num_of_simulations)
    
    # Portfólió létrehozása
    portfolio_returns = weight_spy * simulated_returns_spy + weight_sbux * simulated_returns_sbux

    # VaR
    alpha = 0.05  # Confidence level for VaR
    sorted_returns = np.sort(portfolio_returns)
    var_idx = int(np.floor(alpha * num_of_simulations))
    var = sorted_returns[var_idx]
    VaRs.append(var)

print(VaRs)

plt.plot(correlations, VaRs, marker='o')
plt.title('VaR vs. Correlation')
plt.xlabel('Correlation')
plt.ylabel('VaR')
plt.grid(True)
plt.show()

#3. feladat (EWMA Estimation of Variance)

import random
random.seed(31415)
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Historikus adatok betöltése
etf_prices = pd.read_csv("SPY.csv", parse_dates=['Date'])

# Hozamszámítás
etf_returns = np.log(etf_prices['Adj Close'].pct_change().dropna()+1)

# Autograde Function: calculate_ewma_variance
def calculate_ewma_variance(df_etf_returns, decay_factor, window):
    ewma_variance = df_etf_returns.ewm(alpha=1-decay_factor, min_periods=window).var()
    return ewma_variance

# EVMA variancia különböző "decay factor" mellett
decay_factors = [0.94, 0.97]
window_size = 100

for decay_factor in decay_factors:
    ewma_variances = calculate_ewma_variance(etf_returns, decay_factor, window_size)
    plt.plot(ewma_variances, label=f'Decay Factor: {decay_factor}')

#Plot formázása
plt.title('EWMA Variance')
plt.xlabel('Time')
plt.ylabel('Variance')
plt.legend()
plt.show()

#4-es feladat (Machine Learning for Risk (Variance) Prediction

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error

# Historikus adatok betöltése
asset_prices = pd.read_csv('SPY.csv', parse_dates=['Date'])

# Napi loghozamok és négyzetes loghozamok kiszámítása
asset_returns = np.log(asset_prices['Adj Close'].pct_change().dropna()+1)
squared_returns = asset_returns**2

# Lag beállítása
lags = range(5, 16)  # Lags between 5 and 15

X = pd.concat([squared_returns.shift(lag) for lag in lags], axis=1)
X.columns = [f'Lag_{lag}' for lag in lags]
X = X.dropna()
y = squared_returns.loc[X.index]

# Modell
model = LinearRegression()

# Idősoros keresztvalidáció
tscv = TimeSeriesSplit(n_splits=5)
mse_scores = []

for train_index, test_index in tscv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    #Tanítóadatbázison
    model.fit(X_train, y_train)

    #Tesztadatbázison
    y_pred = model.predict(X_test)

    # MSE kiszámítása
    mse = mean_squared_error(y_test, y_pred)
    mse_scores.append(mse)

# MSE
mean_mse = np.mean(mse_scores)
print (mse_scores)
print(f"Mean Squared Error (MSE): {mean_mse:.4f}")
